----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

Parallelism

----------------------------------------------------------------------------------------------------

Sections:
1. DEFINITIONS
2. PITFALLS

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

1. DEFINITIONS

----------------------------------------------------------------------------------------------------

Concurrency: 

ex. process web requests all at once instead one at a time (1 thread)

- general:
    - many things at once (responsiveness, multiple times)
    - Can occur over a single processor (interleaving execution)

- essentially:
    - Overlapping Execution: Tasks appear to run simultaneously but may or may not be executed at 
      the same physical moment. 
    - Resource Sharing: Concurrent systems often involve multiple tasks sharing and accessing 
      resources 
      such as memory, data, files, etc.
    - Focus on State Management: Concurrency requires careful handling of shared states to prevent 
      issues like race conditions or deadlocks.

- strategies:
    - Immutability: Use immutable objects which can't be modified after their creation. Immutable 
      objects are inherently thread-safe and can help in avoiding synchronization issues.
    - Thread-local Storage: Use thread-local storage to avoid sharing data between threads, which 
      means each thread has its own copy of the data.
    - Minimize Locking Scope: Keep the scope of locking as small and as quick as possible to reduce 
      contention and improve performance.
    - Use Higher-Level Abstractions: Whenever possible, use higher-level concurrency abstractions 
      like those provided by concurrency libraries, which manage low-level details and reduce the 
      chance of errors.

----------------------------------------------------------------------------------------------------

Parallelism:

ex. distribute web requests across different processors to handle web at same time (2+ threads)

- general:
    - subset of concurrency
    - requires multiple processing units
    - many things at the same time (speed)

- essentially:
    - True Simultaneous Execution: In parallelization, multiple processors or cores are involved, 
      with each processor handling a different part of a larger task at the same time.
    - Efficiency and Performance: The main goal of parallelization is to decrease the time it takes 
      to complete a task by dividing the work among multiple processing units.
    - Applicability to Large Problems: Parallelization is particularly beneficial for 
      computationally intensive tasks or large data processing, where the workload can be divided 
      easily into independent tasks.

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

2. PITFALLS

----------------------------------------------------------------------------------------------------

Pitfalls of both:

- Race Conditions: two or more threads or processes attempt to modify shared data at the same time
  - problems: can lead to inconsistent and unpredictable results
  - solutions:
    - Locks/Mutexes: Use locks to synchronize access to shared resources, ensuring that only one 
      thread can access the resource at a time.
    - Atomic Operations: Use atomic operations provided by many programming languages, which are 
      designed to be performed without interference from other threads.
    - Design Consideration: Restructure code to reduce the shared state or to access shared state in 
      a controlled manner.

- Deadlocks: two or more processes or threads are waiting on each other to release resources they 
  need to continue 
  - problems: cycle of dependencies that prevents any of them from proceeding (ex. infinite loops)
  - solutions: 
    - Lock Ordering: Always acquire locks in a consistent order among different threads.
    - Lock Timeout: Implement lock timeouts. Threads attempt to acquire a lock but give up if they 
      can't do so within a certain timeframe.
    - Resource Allocation Graphs: Detect cycles in resource allocation graphs as deadlocks occur 
      when these cycles are present.
    - Try-Lock Techniques: Use non-blocking lock attempts (try-lock). If a lock cannot be acquired, 
      release all currently held locks, wait, and try again.

- Synchronization issues: lost updates, stale data, and thread interference
  - solutions:
    - Condition Variables:uUse along with mutexes to manage the synchronization between threads 
      more precisely.
    - Barriers: synchronize threads at a certain point before any of them can proceed.
    - Semaphore: control access to common resources by multiple threads & maintain the count of the 
      number of available resources.
      - Binary Semaphore (Mutex): acts as a lock, allowing only one thread at a time to access 
        the resource.
      - Counting Semaphore: allows up to a defined number of threads to access a particular 
        resource simultaneously.

----------------------------------------------------------------------------------------------------

Pitfalls of Parallelism:

- Data Splitting: data divided amongst multiple processing units
    - problems: processor load imbalance, excessive inter-processor communication
    - solutions:
        - split data into nearly equal, independent chunks where possible
        - dynamic scheduling: assign tasks to processors as they finish their current tasks

- Load Balancing: all processors or threads have approximately equal amounts of work
    - problems: idle (underutilization) or overloaded (bottlenecks) processors
    - solutions: dynamic load balancing strategies, where workloads are adjusted dynamically 
      based on the current load of each processor.
      - using work-stealing queues where idle processors can 'steal' tasks from busy processors.

- Combining Results from Parallel Tasks: after tasks processed in parallel, combining them
    - problem: complex and time-consuming if not managed correctly, especially if results need to be 
      combined in a specific order or manner.
    - solutions:  reduction operations and concurrent data structures designed for aggregation.
        - ex. MPI Reduce operation/parallel reduction
        - ex. OpenMP: merge results from multiple threads
        - Aggregation: combining multiple pieces of data or computational results to form a single 
          cohesive output
          
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
