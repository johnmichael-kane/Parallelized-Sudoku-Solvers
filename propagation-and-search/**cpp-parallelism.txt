----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

C++ Parallelism Techniques and Tools

----------------------------------------------------------------------------------------------------

Sections:
1. GENERAL
2. STD LIBRARY ** (built in C++ support)
3. SINGLE MACHINE LIBRARIES ** (OpenMB easiest)
4. ADVANCED LIBRARIES
5. AGGREGATION

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

1. GENERAL

----------------------------------------------------------------------------------------------------

Synchronous:

Asynchronous: 

Scheduler: component of the OS responsible for managing allocation of CPU time to various processes 
& threads running on a computer 
- key functions: task switching, load balancing, priority management, deadlock/live-lock prevention

Parallelism technique use cases:

- For fine-grained control and systems programming:
    - low-level APIs like POSIX threads or the standard thread library are appropriate.
- For data parallelism and task-based parallelism in desktop applications:
    - OpenMP, Intel TBB, and C++20 concurrency features offer high-level abstractions that simplify
      development.
- For large-scale distributed applications:
    - MPI provides the necessary tools to manage complex communications and data distribution 
      across many nodes.

Commonly used with parallelism:

- std::vector
    - dynamic array
    - Ex. 
      std::vector<std::thread> threads; // arr of threads
      for (int i = 0; i < 10; ++i) threads.push_back(std::thread(increment_counter)); // add arr end

- std::chrono
    - high precision clocks
    - time points & durations
    - convert various time units
    - Ex.
      auto start = std::chrono::high_resolution_clock::now(); // get curr time
      // do work
      auto end = std::chrono::high_resolution_clock::now(); // get curr time
      std::chrono::duration<double> elapsed = end - start; // calc time elapsed
      return elapsed.count(); // return time elapsed calculation
    - Ex. 
      auto now() { return std::chrono::steady_clock::now(); } // declare at top of file
      const auto start{now()};
      auto awake_time()
      {
          using std::chrono::operator""ms;
          return now() + 2000ms;
      }
      std::chrono::duration<double, std::milli> elapsed{now() - start};

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

2. STD LIBRARY

Standard Library Multithreading

Concurrency support library (C++): https://en.cppreference.com/w/c/thread

----------------------------------------------------------------------------------------------------

<thread> 
https://en.cppreference.com/w/cpp/header/thread

- include <compare> (==, !=, <, <=, >, >=, <=>)
    - thread1 <=> thread2 [comparison operator] [comparison condition] returns true/false
    - Ex.
      std::cout << (p1 <=> p3 == 0) << std::endl;  // True, because p1 is equal to p3

- std:: this_thread (access current thread execution)

    - allow thread to sleep for specified duration, temporarily suspending its execution
        - useful for reducing CPU usage/synchronizing threads
        - Ex. (Sleep For)
          std::this_thread::sleep_for(std::chrono::milliseconds(500));
        - Ex. (Sleep Until)
          std::this_thread::sleep_until([user specified]);


    - yield thread execution & allow other threads to run (help avoid busy waiting for a condition)
        - instead of forcing sleep, HINT scheduler that it's an opportune moment to switch
        - can decide to continue executing current thread if no other threads that need to run
        - Ex. (Yield)
          std::this_thread::yield();  // Encourage the scheduler to switch to other tasks

- std:: thread (manage seperate thread)

    - create and manage threads directly
    
        - Ex. (Member Function)
            std::thread threads[10]; // array that holds 10 thread objects
            for (int i = 0; i < 10; ++i)
                threads[i] = std::thread(print_id, i + 1); // thread(control, [control's argument(s)])
            for (auto& th : threads) th.join(); // join all threads (wait for all to finish execution)

        - Ex. (Lambda)
            std::thread t([](){
                std::cout << "Thread executing" << std::endl;
                std::this_thread::sleep_for(std::chrono::milliseconds(100));  // simulate work
            });
            t.join();

        - Ex.
          int n = 0;
          foo f;
          baz b;
          std::thread t1; // t1 is not a thread
          std::thread t2(f1, n + 1); // pass by value
          std::thread t3(f2, std::ref(n)); // pass by reference
          std::thread t4(std::move(t3)); // t4 is now running a function, t3 is no longer a thread
          std::thread t5(&foo::bar, &f); // t5 runs foo::bar() on object f
          std::thread t6(b); // t6 runs baz::operator() on a copy of object b
          t2.join();
          t4.join();
          t5.join();
          t6.join();

    - id 
        - unique identifier
        - Ex. (.get_id())
            - Get Id: 
            .get_id(); (get id)
            - Formatter: (convert thread to textual representation)
            std::cout << std::format("current thread id: {}\n", this_id);
            - Hasher: 
            std::hash<std::thread::id> hasher; // create hash thread
            hasher(t.get_id()) // hash thread id
        - Ex. (std::id)
          std::thread::id identifier = num;

    - swap
        - swap threads
        - Ex. (t1.swap(t2);)
          using std::swap;
          std::thread t1(foo);
          std::thread t2(bar);
          swap(t1, t2);
        - Ex. (thread.swap(thread))
          t1.swap(t2);

    - detach (.detach())
        - Separates the thread of execution from the thread object, allowing execution to continue 
          independently. Any allocated resources will be freed once the thread exits. 
        

----------------------------------------------------------------------------------------------------

<mutex> 
Review https://en.cppreference.com/w/cpp/thread 
Specifically "Mutual exclusion" and "Generic mutex management"

Mutexes and Locks synchronize access to shared resources with: 

- std::mutex 
    - BASIC: lock shared resource on thread access, block access until unlocked

    - Ex. (Basic Locking)
      std::mutex mtx;
      mtx.lock();
      // do something (critical section)
      mtx.unlock();

- std::recursive_mutex 
    - RECURSION: allow same thread to lock multiple times without deadlock (keep 
      count of # of times locked: lock same # times unlocked)

    - Ex. (Recursive Locking)
      std::recursive_mutex rec_mtx;
      rec_mtx.lock();
      if (depth > 0) { 
        recursive_access(depth - 1);  // Recursive call
      }
      rec_mtx.unlock();

- std::lock_guard 
    - SAFEGUARD: own a thread process until lock released

    - Ex. 
      void safe_increment() {
          std::lock_guard<std::mutex> guard(mtx);
          shared_data++;  // The mutex is automatically released when guard goes out of scope
      }

- std::unique_lock 

    - FLEXIBLE: repeatedly lock & unlock multiple times
    - typically used with condition variables

    - Ex. (Deferred Locking)
        std::mutex mtx; // define mutex mtx 
        std::unique_lock<std::mutex> ulock(mtx, std::defer_lock); // Mutex not locked here
        ulock.lock(); // Now we lock the mutex
        ulock.unlock(); // Unlock manually if needed before the end of the scope
        ulock.lock(); // Lock again if necessary

    - Ex. (Conditional Locking)
        std::unique_lock<std::mutex> ulock(mtx);
        while (!data_ready) {
            ulock.unlock(); // wait for data to be ready
            // do something
            ulock.lock(); // Lock again to check the condition
        }

    - Ex. (Using Condition Variables)
        std::unique_lock<std::mutex> ulock(mtx);
        while (!ready) cv.wait(ulock); // unlocks mutex and waits, then locks it back when signaled

----------------------------------------------------------------------------------------------------

<condition_variable> 
Review https://en.cppreference.com/w/cpp/thread 
Specifically "Condition variables"

- std::condition_variable.
    - manage the execution sequence of multiple threads by allowing them to wait for specific 
      conditions or events.

    - Ex.
      std::condition_variable cv;
      
      cv.notify_one();  // Notify one waiting thread
      cv.notify_all();  // Notify all waiting Threads
      cv.wait(lock, []{ return data_ready; });  // Wait until data_ready becomes true

----------------------------------------------------------------------------------------------------

<atomic> 
Review https://en.cppreference.com/w/cpp/thread 
Specifically "Operations on atomic types", "Flag types and operations", "Initialization", and
"Memory synchronization ordering"

- std::atomic
    - Lock-Free Operations: Enables safe manipulation of shared data between threads without using 
      mutexes, reducing the overhead that comes with locking mechanisms.
    - Memory Order Guarantees: Ensures proper ordering of operations in concurrent environments, 
      which is crucial for maintaining data consistency and correctness.

    - Ex.
      std::atomic<int> counter(0);
      counter.fetch_add(1, std::memory_order_relaxed);  // Increment atomically

----------------------------------------------------------------------------------------------------

<future> 
Review https://en.cppreference.com/w/cpp/thread 
Specifically "Futures"

Manage asynchronous operations using:

- std::async 
    - run function asynchronously (potentially in a new thread) & return future (hold result)
    
    - Ex. 
      int compute() { return 42; }
      auto result = std::async(compute);
      std::cout << "Result: " << result.get(); // Blocks until 'compute' done & returns result

- std::future 
    - access the result of asynchronous operations
    
    - Ex. 
      std::future<int> futureResult = std::async(std::launch::async, [](){ return 123; });
      int value = futureResult.get();  // Wait for the result

- std::promise
    - store value type T that can be retrieve by a future (asynchronous transfer betweeen threads)
    
    - Ex. 
      std::promise<int> prom; // example use
      std::future<int> fut = prom.get_future(); // get promise
      std::thread producer([&prom]() { prom.set_value(100); }); // define thread to get and set prom
      std::cout << "Value from promise: " << fut.get();  // Wait and retrieve the value
      producer.join(); // wait for thread to finish execution

----------------------------------------------------------------------------------------------------

C++ Parallel Algorithms (C++17 and later):

- Standard Algorithms: Execute standard algorithms (like sort, for_each) in parallel.
- Execution Policies: Specify how algorithms should be executed (sequentially, in parallel, or in 
  parallel and vectorized).

----------------------------------------------------------------------------------------------------

C++20 Concurrency Features:

- Jthreads: Enhanced thread management with interruption support.
- Latches and Barriers: New synchronization primitives that facilitate certain types of coordination 
  between threads.
- Semaphores: Manage access counts for resources, allowing more flexible synchronization than 
  mutexes.
- Coroutines: Introduce a new model for asynchronous programming that can be used to handle 
  operations without blocking threads.

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

SINGLE MACHINE LIBRARIES

----------------------------------------------------------------------------------------------------

<omp.h>

OpenMP Syntax reference guide: https://www.openmp.org/wp-content/uploads/OpenMPRefGuide-5.2-Web-2024.pdf

Use Case: Single machine

Overview: 
- Compiler Directives: Utilize pragmas to automatically parallelize loops and sections of code 
(#pragma omp).
- Built-in Functions: Leverage OpenMP functions for thread management and synchronization.
- Scalability: Easily scale up with the number of processor cores available.

Key Features:
- Directives and Constructs:
  - including parallel, for loops, sections, single, master, critical, atomic, and flush, etc. 
  - Each directive facilitates a different aspect of parallel programming such as creating parallel 
    regions, dividing work among threads, and managing access to shared resources.
- Data Environment:
  - The guide discusses how data is handled in parallel regions, detailing clauses like private, 
    firstprivate, shared, reduction, and more. 
  - These clauses define how variables are shared or duplicated across threads.
- Synchronization:
    - Various tools for coordinating threads, such as critical, barrier, atomic, and ordered 
      constructs, which help ensure that parallel tasks run smoothly without data corruption or 
      race conditions.
- Memory Model:
    - OpenMP's memory model is explained, including how memory is allocated and managed across 
      threads and how it impacts the performance and correctness of parallel programs.
- Tasking:
    - The tasking model allows more flexible decompositions of work that can dynamically adjust to 
      runtime conditions. Constructs like task, taskyield, taskwait, and taskgroup are covered.
- Device Offloading:
    - For targeting accelerators like GPUs, OpenMP includes directives for offloading computation to 
      these devices. This includes target, teams, and distribute constructs.

Safety & Control Features:
- Parallel Regions:
    - create parallel regions with #pragma omp parallel. Inside these regions, the workload is 
      automatically distributed among multiple threads. The runtime environment manages thread 
      creation, execution, and termination, which abstracts much of the complexity involved in 
      thread management.
- Data Scope Attributes:
    - explicit control over how data is shared among threads:
        - shared: Variables are shared among all threads.
        - private: Each thread gets a private copy of the variable.
        - firstprivate: Each thread gets a private copy of the variable, initialized with the value 
          of the variable as it existed before entering the parallel region.
        - lastprivate: Similar to firstprivate, but also assigns the last iteration's variable value 
          back to the original variable after the region.
    - These directives prevent data races by controlling access to variables.
- Synchronization Constructs:
    - constructs for sequencing threads & preventing race conditions + ensuring correct output:
        - critical: Ensures that the code block inside the construct is executed by only one thread at 
        a time.
        - atomic: Provides a simpler and faster form of synchronization for certain operations on shared 
        memory, suitable for updating a single variable or bit location without using mutex.
        - barrier: A synchronization point where each thread waits until all threads in the team reach 
        this point.
        - ordered: Ensures that the execution of code blocks in the ordered region is done in the order 
        of the loop iterations.
    - These constructs allow threads to safely
- Loop Work Sharing:
    - #pragma omp for or #pragma omp do: 
        - These directives distribute loop iterations across the team of threads.
    - Clauses like schedule can be used to define how iterations are divided (statically, dynamically).
- Task-Based Parallelism:
    - OpenMP supports creating tasks with #pragma omp task, which allows more granular and flexible 
      handling of work units that can execute concurrently.
    - This model is particularly useful for dynamic and irregular workloads where predefining the 
      work distribution is less effective.

----------------------------------------------------------------------------------------------------

<tbb/tbb.h>

Intel Threading Building Blocks (TBB):

PDF guide: https://www.intel.com/content/www/us/en/docs/onetbb/developer-guide-api-reference/2021-6/overview.html

Use Case: Single machine

Overview:
- Task-based Parallelism: Decompose work into tasks that run on a managed thread pool.
- Concurrency-safe Containers: Access built-in thread-safe containers and algorithms.
- Load Balancing: Automatically distribute tasks among threads for optimal performance.

Key Features:
1. Task-based Parallelism: allow TBB scheduler to efficiently manage CPU resources
2. Scalable Containers: concurrent containers designed to be used safely by multiple threads
- concurrent_hash_map
- concurrent_queue
- concurrent_vector
3. Parallel Algorithms: automatically partition data & execute algorithm in parallel
- parallel_for
- parallel_reduce
- parallel_sort
4. Flow Graph
- Expressing concurrency as a graph of dependent tasks. 
- This is particularly useful for defining and controlling complex task-based dependencies.
5. Memory Allocator: avoid bottlenecks related to dynamic memory allocation
- tbb::scalable_allocator

#include <tbb/tbb.h>

// parallelize loops performing independent iterations

#include <tbb/parallel_for.h>

void process(std::vector<int>& data) {
    tbb::parallel_for(tbb::blocked_range<size_t>(0, data.size()),
                      [&data](const tbb::blocked_range<size_t>& range) {
                          for (size_t i = range.begin(); i != range.end(); ++i) {
                              data[i] *= 2;  // Example operation
                          }
                      });
}

// complex dependencies or asynchronous task execution

#include <tbb/task_group.h>

void taskA();
void taskB();

void executeTasks() {
    tbb::task_group g;
    g.run(taskA);  // Executes taskA asynchronously
    g.run(taskB);  // Executes taskB asynchronously
    g.wait();      // Waits for both tasks to complete
}

// manage shared data if your program involves shared data structures accessed by multiple threads.

#include <tbb/concurrent_vector.h>

tbb::concurrent_vector<int> my_vector;
my_vector.push_back(42);  // Safe to use concurrently

// if heavy dynamic memory allocation, scalable allocator to improve memory management performance

#include <tbb/scalable_allocator.h>

void* operator new(size_t size) {
    return scalable_malloc(size);
}
void operator delete(void* ptr) noexcept {
    scalable_free(ptr);
}

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

ADVANCED LIBRARIES

----------------------------------------------------------------------------------------------------

Boost.Asio (for Asynchronous Programming):

<boost/asio.hpp>

Overview:
- Asynchronous I/O: Manage asynchronous input/output operations, which can be useful in network 
  programming and file handling.
- Timers and Sockets: Utilize asynchronous timers and sockets within a multi-threading context.

Key Features:
1. Asynchronous and Synchronous I/O Operations:
2. Proactor and Reactor Design Patterns:
3. Scalability:
4. Portability:
5. Timer and Event Handling:
6. Integration with Other Boost Libraries:

Use Cases: 
1. Networked Applications:
2. Real-Time Data Processing:
3. High-Performance I/O:
4, Concurrency Handling:

----------------------------------------------------------------------------------------------------

MPI (Message Passing Interface):

Overview:
- Distributed Computing: Used primarily for parallel programming on distributed memory systems 
  (clusters)
- Scalability: Effective for large-scale parallel applications that run on hundreds or thousands of 
  nodes.

Use Cases:
- several machines simultaneously

----------------------------------------------------------------------------------------------------

POSIX Threads (Pthreads):

- Cross-platform: Available on many types of UNIX-like operating systems.
- Low-level Control: Direct control over thread creation, synchronization, and management.
- Manual Management: Requires manual handling of threads, mutexes, and other synchronization 
  mechanisms.

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

5. AGGREGATION

----------------------------------------------------------------------------------------------------

MPI's Reduce Operation:

- Does: aggregates data from all members of a group (a set of process elements) into a single 
  result available at the root process. The operation is performed in a way that is optimized for 
  distributed systems.
- Common Uses: sum, max, min, and logical operation
    - ex. calculate the sum of all elements of an array distributed across multiple processes, 
      each process computes a local sum, and MPI's Reduce will combine these into a global sum.
- Efficiency: designed to minimize data movement and maximize parallel hardware utilization, using 
  tree-based reduction techniques to aggregate data across a network with minimal latency.

----------------------------------------------------------------------------------------------------

Parallel Reduction in OpenMP:

- Does: provides a reduction clause in its parallel and loop constructs, allowing automatic 
  aggregation of results from multiple threads. Each thread performs its assigned task and its 
  partial result is combined with others using a specified operation (like addition, multiplication, 
  etc.). 
- Examples:
    - #pragma omp parallel for reduction(+:sum)
      for (int i = 0; i < 100; i++) {
          sum += a[i];
      }
      // 'sum' now contains the total sum of array 'a'
        -  Efficiency: The reduction clause in OpenMP minimizes the overhead of synchronization by
           allowing each thread to work with a local copy of the variable being reduced, combining 
           them only at the end of the parallel section, thus reducing the contention on shared 
           resources.
